{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Vaccination data processing\n",
    "This notebook shows the code that was run once to download and adapt the data on vaccination roll-out in Australia\n",
    "used in the vaccination extension to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from aust_covid.constants import set_project_base_path\n",
    "\n",
    "project_paths = set_project_base_path(\"../\")\n",
    "RUNS_PATH = project_paths[\"RUNS_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape links to pages that house link for vaccine data\n",
    "url = \"https://www.health.gov.au/resources/collections/covid-19-vaccination-vaccination-data?language=en\"\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, \"html.parser\")\n",
    "urls = []\n",
    "for link in soup.find_all(\"a\"):\n",
    "    urls.append(link.get(\"href\"))\n",
    "data_urls = [\"https://www.health.gov.au\" + u for u in urls if \"/resources/publications\" in u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the urls that download the excel file\n",
    "excel_urls = []\n",
    "for d in data_urls:\n",
    "    data_reqs = requests.get(d)\n",
    "    data_soup = BeautifulSoup(data_reqs.text, \"html.parser\")\n",
    "    download_urls = []\n",
    "    for link in data_soup.find_all(\"a\"):\n",
    "        download_urls.append(link.get(\"href\"))\n",
    "    download = [\"https://www.health.gov.au\" + u for u in download_urls if \"xlsx\" in u]\n",
    "    excel_urls.append(download[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter url download list to exclude those from 2023\n",
    "keywordfilter = set([\"2023\"])\n",
    "excel_urls_21_22 = [u for u in excel_urls if not any(word in u for word in keywordfilter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe, transpose and add date column\n",
    "df = pd.DataFrame()\n",
    "for (counter, l) in enumerate(excel_urls_21_22):\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    temp_df = pd.read_excel(l)\n",
    "    temp_df = temp_df.rename(columns={temp_df.columns[0]:\"variable\", temp_df.columns[1]:\"value\"})\n",
    "    t_df = temp_df[[\"variable\", \"value\"]].set_index(\"variable\").T\n",
    "    t_df = t_df. dropna(axis=1)\n",
    "    t_df.reset_index()\n",
    "    # select second date when two dates exist\n",
    "    date = re.findall(r\"(?<=data-).*?(?<=\\d{4})\", l)\n",
    "    if len(date)>1:\n",
    "        t_df[\"date\"] = pd.to_datetime(date[1])\n",
    "    else:\n",
    "        t_df[\"date\"] = pd.to_datetime(date)\n",
    "    print(l)\n",
    "    df = pd.concat([df, t_df], axis=0, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
